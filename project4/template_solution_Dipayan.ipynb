{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57002395eb6b50f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46aca0e5d9ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c6ff7632991155",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from transformers import GPT2Config, GPT2Tokenizer, GPT2Model\n",
    "from transformers import AlbertConfig, AlbertTokenizer, AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9b5e89d4b3a02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\")\n",
    "BATCH_SIZE = 20  # TODO: Set the batch size according to both training performance and available memory\n",
    "NUM_EPOCHS = 50  # TODO: Set the number of epochs\n",
    "train_val = pd.read_csv(\"train.csv\")\n",
    "test_val = pd.read_csv(\"test_no_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fef543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b5990d1db64be6a070932483949b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff093dea115246e2982f24c522c8682c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f3b222d01e4a2681b8a7087d5f399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87b7db0626d494a885b74d684044f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ca43fd7fc44018ac8c615de6f06e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/893M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `mps`\n"
     ]
    }
   ],
   "source": [
    "print('Loading configuration...')\n",
    "model_config = AlbertConfig.from_pretrained(pretrained_model_name_or_path=\"albert-xxlarge-v2\")\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AlbertTokenizer.from_pretrained(pretrained_model_name_or_path=\"albert-xxlarge-v2\")\n",
    "# default to right padding\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = AlbertModel.from_pretrained(pretrained_model_name_or_path=\"albert-xxlarge-v2\", config=model_config)\n",
    "# resize model embedding to match new tokenizer\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# Load model to defined device.\n",
    "model.to(DEVICE)\n",
    "print('Model loaded to `%s`'%DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161fdafaedaa5b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fill out ReviewDataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        self.text = data_frame[\"title\"].str.cat(data_frame[\"sentence\"], sep = \" \")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"text\": self.text[index]}\n",
    "\n",
    "# class ReviewDataset(Dataset):\n",
    "#     def __init__(self, data_frame):\n",
    "#         self.title = data_frame[\"title\"]\n",
    "#         self.sentence = data_frame[\"sentence\"]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.title)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return {\"title\": self.title[index], \"sentence\": self.sentence[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676f7a012f50e988",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n",
      "Created `train_dataset` with 12500 samples\n",
      "Created `train_loader` with 625 batches\n",
      "\n",
      "Dealing with Test...\n",
      "Created `test_dataset` with 1000 samples\n",
      "Created `test_loader` with 50 batches\n"
     ]
    }
   ],
   "source": [
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = ReviewDataset(train_val)\n",
    "print('Created `train_dataset` with %d samples'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=0, pin_memory=True)\n",
    "print('Created `train_loader` with %d batches'%len(train_loader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Test...')\n",
    "# Create pytorch dataset.\n",
    "test_dataset = ReviewDataset(test_val)\n",
    "print('Created `test_dataset` with %d samples'%len(test_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=0, pin_memory=True)\n",
    "print('Created `test_loader` with %d batches'%len(test_loader))\n",
    "# Additional code if needed\n",
    "embedding_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef734414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_test = np.zeros((len(test_dataset), 2*embedding_size))\n",
    "# i = 0\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "#         data = list(batch.values())\n",
    "#         encoded_titles = tokenizer(data[0], return_tensors='pt', padding=True, truncation=False)\n",
    "#         encoded_sentences = tokenizer(data[1], return_tensors='pt', padding=True, truncation=False)\n",
    "#         del data\n",
    "#         encoded_titles.to(DEVICE)\n",
    "#         encoded_sentences.to(DEVICE)\n",
    "#         outputs_titles = model(**encoded_titles)\n",
    "#         outputs_sentences = model(**encoded_sentences)\n",
    "#         word_embeddings_titles = outputs_titles.last_hidden_state * encoded_titles.attention_mask.unsqueeze(-1).float()\n",
    "#         word_embeddings_sentences = outputs_sentences.last_hidden_state * encoded_sentences.attention_mask.unsqueeze(-1).float()\n",
    "#         del outputs_titles\n",
    "#         del outputs_sentences\n",
    "#         sentence_embeddings_titles = word_embeddings_titles.sum(dim=1)\n",
    "#         sentence_embeddings_sentences = word_embeddings_sentences.sum(dim=1)\n",
    "#         del word_embeddings_titles\n",
    "#         del word_embeddings_sentences\n",
    "#         sentence_embeddings_titles /= encoded_titles.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "#         sentence_embeddings_sentences /= encoded_sentences.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "#         del encoded_titles\n",
    "#         del encoded_sentences\n",
    "#         embeddings_test[BATCH_SIZE*i : BATCH_SIZE*(i+1)] = np.hstack([sentence_embeddings_titles.cpu().numpy(), sentence_embeddings_sentences.cpu().numpy()])\n",
    "#         del sentence_embeddings_titles\n",
    "#         del sentence_embeddings_sentences\n",
    "#         i +=1\n",
    "# np.save('embeddings_test.npy', embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_train = np.zeros((len(train_dataset), 2*embedding_size))\n",
    "# i = 0\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "#         data = list(batch.values())\n",
    "#         encoded_titles = tokenizer(data[0], return_tensors='pt', padding=True, truncation=False)\n",
    "#         encoded_sentences = tokenizer(data[1], return_tensors='pt', padding=True, truncation=False)\n",
    "#         del data\n",
    "#         encoded_titles.to(DEVICE)\n",
    "#         encoded_sentences.to(DEVICE)\n",
    "#         outputs_titles = model(**encoded_titles)\n",
    "#         outputs_sentences = model(**encoded_sentences)\n",
    "#         word_embeddings_titles = outputs_titles.last_hidden_state * encoded_titles.attention_mask.unsqueeze(-1).float()\n",
    "#         word_embeddings_sentences = outputs_sentences.last_hidden_state * encoded_sentences.attention_mask.unsqueeze(-1).float()\n",
    "#         del outputs_titles\n",
    "#         del outputs_sentences\n",
    "#         sentence_embeddings_titles = word_embeddings_titles.sum(dim=1)\n",
    "#         sentence_embeddings_sentences = word_embeddings_sentences.sum(dim=1)\n",
    "#         del word_embeddings_titles\n",
    "#         del word_embeddings_sentences\n",
    "#         sentence_embeddings_titles /= encoded_titles.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "#         sentence_embeddings_sentences /= encoded_sentences.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "#         del encoded_titles\n",
    "#         del encoded_sentences\n",
    "#         embeddings_train[BATCH_SIZE*i : BATCH_SIZE*(i+1)] = np.hstack([sentence_embeddings_titles.cpu().numpy(), sentence_embeddings_sentences.cpu().numpy()])\n",
    "#         del sentence_embeddings_titles\n",
    "#         del sentence_embeddings_sentences\n",
    "#         i +=1\n",
    "# np.save('embeddings_train.npy', embeddings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b693ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:15<00:00,  7.51s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings_test = np.zeros((len(test_dataset), embedding_size))\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        data = list(batch.values())\n",
    "        encoded = tokenizer(*data, return_tensors='pt', padding=True, truncation=False)\n",
    "        del data\n",
    "        encoded.to(DEVICE)\n",
    "        outputs = model(**encoded)\n",
    "        word_embeddings = outputs.last_hidden_state * encoded.attention_mask.unsqueeze(-1).float()\n",
    "        del outputs\n",
    "        sentence_embeddings = word_embeddings.sum(dim=1)\n",
    "        del word_embeddings\n",
    "        sentence_embeddings /= encoded.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "        del encoded\n",
    "        embeddings_test[BATCH_SIZE*i : BATCH_SIZE*(i+1)] = sentence_embeddings.cpu().numpy()\n",
    "        del sentence_embeddings\n",
    "        i +=1\n",
    "np.save('embeddings_test.npy', embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156f8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [1:00:39<00:00,  5.82s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings_train = np.zeros((len(train_dataset), embedding_size))\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "        data = list(batch.values())\n",
    "        encoded = tokenizer(*data, return_tensors='pt', padding=True, truncation=False)\n",
    "        del data\n",
    "        encoded.to(DEVICE)\n",
    "        outputs = model(**encoded)\n",
    "        word_embeddings = outputs.last_hidden_state * encoded.attention_mask.unsqueeze(-1).float()\n",
    "        del outputs\n",
    "        sentence_embeddings = word_embeddings.sum(dim=1)\n",
    "        del word_embeddings\n",
    "        sentence_embeddings /= encoded.attention_mask.sum(dim=1, keepdim=True).float()\n",
    "        del encoded\n",
    "        embeddings_train[BATCH_SIZE*i : BATCH_SIZE*(i+1)] = sentence_embeddings.cpu().numpy()\n",
    "        del sentence_embeddings\n",
    "        i +=1\n",
    "np.save('embeddings_train.npy', embeddings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c83c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y=None, train=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=10):\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf22a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = np.load('embeddings_train.npy')\n",
    "# embeddings_train = (embeddings_train - np.mean(embeddings_train, axis=1)[:, np.newaxis]) / np.std(embeddings_train, axis=1)[:, np.newaxis]\n",
    "\n",
    "X = embeddings_train\n",
    "y = train_val[\"score\"].to_numpy()\n",
    "train_loader = create_loader_from_np(X[0:round(0.8*X.shape[0])], y[0:round(0.8*len(y))], train=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = create_loader_from_np(X[round(0.2*X.shape[0]):], y[round(0.2*len(y)):], train=True, batch_size=BATCH_SIZE)\n",
    "train_loader_final = create_loader_from_np(X, y, train=True, batch_size=BATCH_SIZE)\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "843f38b9dbea00b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fill out MyModule\n",
    "layer1_size = 500\n",
    "layer2_size = 500\n",
    "dropout_prop = 0\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, layer1_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prop)\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_prop)\n",
    "        # self.fc3 = nn.Linear(layer2_size, layer3_size)\n",
    "        # self.dropout3 = nn.Dropout(dropout_prop)\n",
    "        self.fc3 = nn.Linear(layer2_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout2(x)\n",
    "        # x = self.fc3(x)\n",
    "        # x = F.elu(x)\n",
    "        # x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605e5bd0373a1dda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0 -- Epoch 1, Batch 100 -- Epoch 1, Batch 200 -- Epoch 1, Batch 300 -- Epoch 1, Batch 400 -- Epoch 1, training loss 5.774501124382019\n",
      "Epoch 1, validation loss 3.2438979597091673\n",
      "Epoch 2, Batch 0 -- Epoch 2, Batch 100 -- Epoch 2, Batch 200 -- Epoch 2, Batch 300 -- Epoch 2, Batch 400 -- Epoch 2, training loss 3.2635896881103514\n",
      "Epoch 2, validation loss 3.0698712236404417\n",
      "Epoch 3, Batch 0 -- Epoch 3, Batch 100 -- Epoch 3, Batch 200 -- Epoch 3, Batch 300 -- Epoch 3, Batch 400 -- Epoch 3, training loss 2.7518372490882874\n",
      "Epoch 3, validation loss 2.3403093088150024\n",
      "Epoch 4, Batch 0 -- Epoch 4, Batch 100 -- Epoch 4, Batch 200 -- Epoch 4, Batch 300 -- Epoch 4, Batch 400 -- Epoch 4, training loss 2.567378203868866\n",
      "Epoch 4, validation loss 2.2591030093193054\n",
      "Epoch 5, Batch 0 -- Epoch 5, Batch 100 -- Epoch 5, Batch 200 -- Epoch 5, Batch 300 -- Epoch 5, Batch 400 -- Epoch 5, training loss 2.2264364428520205\n",
      "Epoch 5, validation loss 2.2433243871688844\n",
      "Epoch 6, Batch 0 -- Epoch 6, Batch 100 -- Epoch 6, Batch 200 -- Epoch 6, Batch 300 -- Epoch 6, Batch 400 -- Epoch 6, training loss 2.0834879744529724\n",
      "Epoch 6, validation loss 1.727297362422943\n",
      "Epoch 7, Batch 0 -- Epoch 7, Batch 100 -- Epoch 7, Batch 200 -- Epoch 7, Batch 300 -- Epoch 7, Batch 400 -- Epoch 7, training loss 1.982329262161255\n",
      "Epoch 7, validation loss 1.9009567153930664\n",
      "Epoch 8, Batch 0 -- Epoch 8, Batch 100 -- Epoch 8, Batch 200 -- Epoch 8, Batch 300 -- Epoch 8, Batch 400 -- Epoch 8, training loss 1.8752001447677613\n",
      "Epoch 8, validation loss 1.7660288368225097\n",
      "Epoch 9, Batch 0 -- Epoch 9, Batch 100 -- Epoch 9, Batch 200 -- Epoch 9, Batch 300 -- Epoch 9, Batch 400 -- Epoch 9, training loss 1.6977212515830993\n",
      "Epoch 9, validation loss 1.996385649394989\n",
      "Epoch 10, Batch 0 -- Epoch 10, Batch 100 -- Epoch 10, Batch 200 -- Epoch 10, Batch 300 -- Epoch 10, Batch 400 -- Epoch 10, training loss 1.7212616456985474\n",
      "Epoch 10, validation loss 1.489908856868744\n",
      "Epoch 11, Batch 0 -- Epoch 11, Batch 100 -- Epoch 11, Batch 200 -- Epoch 11, Batch 300 -- Epoch 11, Batch 400 -- Epoch 11, training loss 1.642110788345337\n",
      "Epoch 11, validation loss 1.5847149230957032\n",
      "Epoch 12, Batch 0 -- Epoch 12, Batch 100 -- Epoch 12, Batch 200 -- Epoch 12, Batch 300 -- Epoch 12, Batch 400 -- Epoch 12, training loss 1.5351363904476165\n",
      "Epoch 12, validation loss 1.5941996312141418\n",
      "Epoch 13, Batch 0 -- Epoch 13, Batch 100 -- Epoch 13, Batch 200 -- Epoch 13, Batch 300 -- Epoch 13, Batch 400 -- Epoch 13, training loss 1.4858918172359468\n",
      "Epoch 13, validation loss 1.2862891515731811\n",
      "Epoch 14, Batch 0 -- Epoch 14, Batch 100 -- Epoch 14, Batch 200 -- Epoch 14, Batch 300 -- Epoch 14, Batch 400 -- Epoch 14, training loss 1.5216843968391418\n",
      "Epoch 14, validation loss 1.3235214007377625\n",
      "Epoch 15, Batch 0 -- Epoch 15, Batch 100 -- Epoch 15, Batch 200 -- Epoch 15, Batch 300 -- Epoch 15, Batch 400 -- Epoch 15, training loss 1.503885188961029\n",
      "Epoch 15, validation loss 1.3956588376045227\n",
      "Epoch 16, Batch 0 -- Epoch 16, Batch 100 -- Epoch 16, Batch 200 -- Epoch 16, Batch 300 -- Epoch 16, Batch 400 -- Epoch 16, training loss 1.3844564016342162\n",
      "Epoch 16, validation loss 1.2485298058986665\n",
      "Epoch 17, Batch 0 -- Epoch 17, Batch 100 -- Epoch 17, Batch 200 -- Epoch 17, Batch 300 -- Epoch 17, Batch 400 -- Epoch 17, training loss 1.3502734360218047\n",
      "Epoch 17, validation loss 1.2744227485179902\n",
      "Epoch 18, Batch 0 -- Epoch 18, Batch 100 -- Epoch 18, Batch 200 -- Epoch 18, Batch 300 -- Epoch 18, Batch 400 -- Epoch 18, training loss 1.3526754916191102\n",
      "Epoch 18, validation loss 1.1518098001480102\n",
      "Epoch 19, Batch 0 -- Epoch 19, Batch 100 -- Epoch 19, Batch 200 -- Epoch 19, Batch 300 -- Epoch 19, Batch 400 -- Epoch 19, training loss 1.324661701440811\n",
      "Epoch 19, validation loss 1.4230126607894897\n",
      "Epoch 20, Batch 0 -- Epoch 20, Batch 100 -- Epoch 20, Batch 200 -- Epoch 20, Batch 300 -- Epoch 20, Batch 400 -- Epoch 20, training loss 1.2844830464839936\n",
      "Epoch 20, validation loss 2.4100266335487364\n",
      "Epoch 21, Batch 0 -- Epoch 21, Batch 100 -- Epoch 21, Batch 200 -- Epoch 21, Batch 300 -- Epoch 21, Batch 400 -- Epoch 21, training loss 1.2841735775470733\n",
      "Epoch 21, validation loss 1.225784358882904\n",
      "Epoch 22, Batch 0 -- Epoch 22, Batch 100 -- Epoch 22, Batch 200 -- Epoch 22, Batch 300 -- Epoch 22, Batch 400 -- Epoch 22, training loss 1.2032410059928893\n",
      "Epoch 22, validation loss 1.122584760427475\n",
      "Epoch 23, Batch 0 -- Epoch 23, Batch 100 -- Epoch 23, Batch 200 -- Epoch 23, Batch 300 -- Epoch 23, Batch 400 -- Epoch 23, training loss 1.2540855151176453\n",
      "Epoch 23, validation loss 1.3180817932605744\n",
      "Epoch 24, Batch 0 -- Epoch 24, Batch 100 -- Epoch 24, Batch 200 -- Epoch 24, Batch 300 -- Epoch 24, Batch 400 -- Epoch 24, training loss 1.2270649159908296\n",
      "Epoch 24, validation loss 1.357997039604187\n",
      "Epoch 25, Batch 0 -- Epoch 25, Batch 100 -- Epoch 25, Batch 200 -- Epoch 25, Batch 300 -- Epoch 25, Batch 400 -- Epoch 25, training loss 1.2520981746196747\n",
      "Epoch 25, validation loss 1.093980850839615\n",
      "Epoch 26, Batch 0 -- Epoch 26, Batch 100 -- Epoch 26, Batch 200 -- Epoch 26, Batch 300 -- Epoch 26, Batch 400 -- Epoch 26, training loss 1.1167246829032897\n",
      "Epoch 26, validation loss 1.7801529175758362\n",
      "Epoch 27, Batch 0 -- Epoch 27, Batch 100 -- Epoch 27, Batch 200 -- Epoch 27, Batch 300 -- Epoch 27, Batch 400 -- Epoch 27, training loss 1.1551378729343413\n",
      "Epoch 27, validation loss 1.5171433108329773\n",
      "Epoch 28, Batch 0 -- Epoch 28, Batch 100 -- Epoch 28, Batch 200 -- Epoch 28, Batch 300 -- Epoch 28, Batch 400 -- Epoch 28, training loss 1.1180437076091767\n",
      "Epoch 28, validation loss 1.180787518262863\n",
      "Epoch 29, Batch 0 -- Epoch 29, Batch 100 -- Epoch 29, Batch 200 -- Epoch 29, Batch 300 -- Epoch 29, Batch 400 -- Epoch 29, training loss 1.1607965272665024\n",
      "Epoch 29, validation loss 1.1561755131721496\n",
      "Epoch 30, Batch 0 -- Epoch 30, Batch 100 -- Epoch 30, Batch 200 -- Epoch 30, Batch 300 -- Epoch 30, Batch 400 -- Epoch 30, training loss 1.1239676664829255\n",
      "Epoch 30, validation loss 1.0759414374828338\n",
      "Epoch 31, Batch 0 -- Epoch 31, Batch 100 -- Epoch 31, Batch 200 -- Epoch 31, Batch 300 -- Epoch 31, Batch 400 -- Epoch 31, training loss 1.0760034299850463\n",
      "Epoch 31, validation loss 0.9564322049379349\n",
      "Epoch 32, Batch 0 -- Epoch 32, Batch 100 -- Epoch 32, Batch 200 -- Epoch 32, Batch 300 -- Epoch 32, Batch 400 -- Epoch 32, training loss 1.016938720035553\n",
      "Epoch 32, validation loss 3.169674348640442\n",
      "Epoch 33, Batch 0 -- Epoch 33, Batch 100 -- Epoch 33, Batch 200 -- Epoch 33, Batch 300 -- Epoch 33, Batch 400 -- Epoch 33, training loss 1.0468161658763886\n",
      "Epoch 33, validation loss 1.0082843191623687\n",
      "Epoch 34, Batch 0 -- Epoch 34, Batch 100 -- Epoch 34, Batch 200 -- Epoch 34, Batch 300 -- Epoch 34, Batch 400 -- Epoch 34, training loss 1.0509602100372315\n",
      "Epoch 34, validation loss 1.5704860441207886\n",
      "Epoch 35, Batch 0 -- Epoch 35, Batch 100 -- Epoch 35, Batch 200 -- Epoch 35, Batch 300 -- Epoch 35, Batch 400 -- Epoch 35, training loss 1.0512501232862472\n",
      "Epoch 35, validation loss 0.9466831460475922\n",
      "Epoch 36, Batch 0 -- Epoch 36, Batch 100 -- Epoch 36, Batch 200 -- Epoch 36, Batch 300 -- Epoch 36, Batch 400 -- Epoch 36, training loss 1.0472185911655425\n",
      "Epoch 36, validation loss 1.03570823802948\n",
      "Epoch 37, Batch 0 -- Epoch 37, Batch 100 -- Epoch 37, Batch 200 -- Epoch 37, Batch 300 -- Epoch 37, Batch 400 -- Epoch 37, training loss 0.9611804659366607\n",
      "Epoch 37, validation loss 0.9637273536682129\n",
      "Epoch 38, Batch 0 -- Epoch 38, Batch 100 -- Epoch 38, Batch 200 -- Epoch 38, Batch 300 -- Epoch 38, Batch 400 -- Epoch 38, training loss 0.9609653352737427\n",
      "Epoch 38, validation loss 0.94961612200737\n",
      "Epoch 39, Batch 0 -- Epoch 39, Batch 100 -- Epoch 39, Batch 200 -- Epoch 39, Batch 300 -- Epoch 39, Batch 400 -- Epoch 39, training loss 0.9240574598789215\n",
      "Epoch 39, validation loss 1.0349370670795441\n",
      "Epoch 40, Batch 0 -- Epoch 40, Batch 100 -- Epoch 40, Batch 200 -- Epoch 40, Batch 300 -- Epoch 40, Batch 400 -- Epoch 40, training loss 0.9859725355625153\n",
      "Epoch 40, validation loss 2.911267778015137\n",
      "Epoch 41, Batch 0 -- Epoch 41, Batch 100 -- Epoch 41, Batch 200 -- Epoch 41, Batch 300 -- Epoch 41, Batch 400 -- Epoch 41, training loss 0.9461925618171692\n",
      "Epoch 41, validation loss 0.8722410228729248\n",
      "Epoch 42, Batch 0 -- Epoch 42, Batch 100 -- Epoch 42, Batch 200 -- Epoch 42, Batch 300 -- Epoch 42, Batch 400 -- Epoch 42, training loss 0.8755852875471115\n",
      "Epoch 42, validation loss 0.936117839050293\n",
      "Epoch 43, Batch 0 -- Epoch 43, Batch 100 -- Epoch 43, Batch 200 -- Epoch 43, Batch 300 -- Epoch 43, Batch 400 -- Epoch 43, training loss 0.848466661977768\n",
      "Epoch 43, validation loss 1.1551190685272217\n",
      "Epoch 44, Batch 0 -- Epoch 44, Batch 100 -- Epoch 44, Batch 200 -- Epoch 44, Batch 300 -- Epoch 44, Batch 400 -- Epoch 44, training loss 0.9763128310203553\n",
      "Epoch 44, validation loss 1.0585248866558075\n",
      "Epoch 45, Batch 0 -- Epoch 45, Batch 100 -- Epoch 45, Batch 200 -- Epoch 45, Batch 300 -- Epoch 45, Batch 400 -- Epoch 45, training loss 0.9423164298057556\n",
      "Epoch 45, validation loss 1.4182647585391999\n",
      "Epoch 46, Batch 0 -- Epoch 46, Batch 100 -- Epoch 46, Batch 200 -- Epoch 46, Batch 300 -- Epoch 46, Batch 400 -- Epoch 46, training loss 0.8429032903194428\n",
      "Epoch 46, validation loss 0.9884310425758361\n",
      "Epoch 47, Batch 0 -- Epoch 47, Batch 100 -- Epoch 47, Batch 200 -- Epoch 47, Batch 300 -- Epoch 47, Batch 400 -- Epoch 47, training loss 0.8056774767637253\n",
      "Epoch 47, validation loss 0.8839484647750855\n",
      "Epoch 48, Batch 0 -- Epoch 48, Batch 100 -- Epoch 48, Batch 200 -- Epoch 48, Batch 300 -- Epoch 48, Batch 400 -- Epoch 48, training loss 0.8500096928596497\n",
      "Epoch 48, validation loss 0.905944548034668\n",
      "Epoch 49, Batch 0 -- Epoch 49, Batch 100 -- Epoch 49, Batch 200 -- Epoch 49, Batch 300 -- Epoch 49, Batch 400 -- Epoch 49, training loss 0.788615757226944\n",
      "Epoch 49, validation loss 0.8400437641620636\n",
      "Epoch 50, Batch 0 -- Epoch 50, Batch 100 -- Epoch 50, Batch 200 -- Epoch 50, Batch 300 -- Epoch 50, Batch 400 -- Epoch 50, training loss 0.782623379778862\n",
      "Epoch 50, validation loss 0.847382581615448\n",
      "Epoch 51, Batch 0 -- Epoch 51, Batch 100 -- Epoch 51, Batch 200 -- Epoch 51, Batch 300 -- Epoch 51, Batch 400 -- Epoch 51, training loss 0.8238810384273529\n",
      "Epoch 51, validation loss 0.8883667638778686\n",
      "Epoch 52, Batch 0 -- Epoch 52, Batch 100 -- Epoch 52, Batch 200 -- Epoch 52, Batch 300 -- Epoch 52, Batch 400 -- Epoch 52, training loss 0.7684055551290512\n",
      "Epoch 52, validation loss 1.0654492009162904\n",
      "Epoch 53, Batch 0 -- Epoch 53, Batch 100 -- Epoch 53, Batch 200 -- Epoch 53, Batch 300 -- Epoch 53, Batch 400 -- Epoch 53, training loss 0.8463295289993286\n",
      "Epoch 53, validation loss 0.871739944601059\n",
      "Epoch 54, Batch 0 -- Epoch 54, Batch 100 -- Epoch 54, Batch 200 -- Epoch 54, Batch 300 -- Epoch 54, Batch 400 -- Epoch 54, training loss 0.7822750512123108\n",
      "Epoch 54, validation loss 0.7856038082838058\n",
      "Epoch 55, Batch 0 -- Epoch 55, Batch 100 -- Epoch 55, Batch 200 -- Epoch 55, Batch 300 -- Epoch 55, Batch 400 -- Epoch 55, training loss 0.8008175078868865\n",
      "Epoch 55, validation loss 0.929917408323288\n",
      "Epoch 56, Batch 0 -- Epoch 56, Batch 100 -- Epoch 56, Batch 200 -- Epoch 56, Batch 300 -- Epoch 56, Batch 400 -- Epoch 56, training loss 0.839542197918892\n",
      "Epoch 56, validation loss 0.9093096881866455\n",
      "Epoch 57, Batch 0 -- Epoch 57, Batch 100 -- Epoch 57, Batch 200 -- Epoch 57, Batch 300 -- Epoch 57, Batch 400 -- Epoch 57, training loss 0.7374598276376724\n",
      "Epoch 57, validation loss 0.8073908178329467\n",
      "Epoch 58, Batch 0 -- Epoch 58, Batch 100 -- Epoch 58, Batch 200 -- Epoch 58, Batch 300 -- Epoch 58, Batch 400 -- Epoch 58, training loss 0.8464868725061416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m valid_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m valid_loader: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipayan/Documents/introml-projects/intro-ML/project4/template_solution_Dipayan.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(DEVICE)   \n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1444\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/Documents/introml-projects/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.train()\n",
    "model.to(DEVICE)\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 100\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "loss_function = nn.MSELoss(reduction=\"sum\")\n",
    "learn_rate = 0.0003\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for batch_id, (X, y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_id % 100 == 0:\n",
    "            print('Epoch {}, Batch {}'.format(epoch+1, batch_id), end=\" -- \")\n",
    "    epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "    print('Epoch {}, training loss {}'.format(epoch+1, epoch_train_loss))\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader: \n",
    "            model.eval()\n",
    "            X = X.to(DEVICE)   \n",
    "            y = y.to(DEVICE)\n",
    "            output_valid = model(X)\n",
    "            loss_valid = loss_function(torch.flatten(output_valid), y)\n",
    "            valid_loss += loss_valid.item()\n",
    "    epoch_valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    print('Epoch {}, validation loss {}'.format(epoch+1, epoch_valid_loss))\n",
    "\n",
    "    if epoch_valid_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = epoch_valid_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018536fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0 -- Epoch 1, Batch 100 -- Epoch 1, Batch 200 -- Epoch 1, Batch 300 -- Epoch 1, Batch 400 -- Epoch 1, Batch 500 -- Epoch 1, Batch 600 -- Epoch 1, training loss 5.362310281066894\n",
      "Epoch 2, Batch 0 -- Epoch 2, Batch 100 -- Epoch 2, Batch 200 -- Epoch 2, Batch 300 -- Epoch 2, Batch 400 -- Epoch 2, Batch 500 -- Epoch 2, Batch 600 -- Epoch 2, training loss 3.082162197189331\n",
      "Epoch 3, Batch 0 -- Epoch 3, Batch 100 -- Epoch 3, Batch 200 -- Epoch 3, Batch 300 -- Epoch 3, Batch 400 -- Epoch 3, Batch 500 -- Epoch 3, Batch 600 -- Epoch 3, training loss 2.6571406590270996\n",
      "Epoch 4, Batch 0 -- Epoch 4, Batch 100 -- Epoch 4, Batch 200 -- Epoch 4, Batch 300 -- Epoch 4, Batch 400 -- Epoch 4, Batch 500 -- Epoch 4, Batch 600 -- Epoch 4, training loss 2.4696303201293945\n",
      "Epoch 5, Batch 0 -- Epoch 5, Batch 100 -- Epoch 5, Batch 200 -- Epoch 5, Batch 300 -- Epoch 5, Batch 400 -- Epoch 5, Batch 500 -- Epoch 5, Batch 600 -- Epoch 5, training loss 2.0968327576828\n",
      "Epoch 6, Batch 0 -- Epoch 6, Batch 100 -- Epoch 6, Batch 200 -- Epoch 6, Batch 300 -- Epoch 6, Batch 400 -- Epoch 6, Batch 500 -- Epoch 6, Batch 600 -- Epoch 6, training loss 1.9515992067718506\n",
      "Epoch 7, Batch 0 -- Epoch 7, Batch 100 -- Epoch 7, Batch 200 -- Epoch 7, Batch 300 -- Epoch 7, Batch 400 -- Epoch 7, Batch 500 -- Epoch 7, Batch 600 -- Epoch 7, training loss 1.910500865135193\n",
      "Epoch 8, Batch 0 -- Epoch 8, Batch 100 -- Epoch 8, Batch 200 -- Epoch 8, Batch 300 -- Epoch 8, Batch 400 -- Epoch 8, Batch 500 -- Epoch 8, Batch 600 -- Epoch 8, training loss 1.7450956382751466\n",
      "Epoch 9, Batch 0 -- Epoch 9, Batch 100 -- Epoch 9, Batch 200 -- Epoch 9, Batch 300 -- Epoch 9, Batch 400 -- Epoch 9, Batch 500 -- Epoch 9, Batch 600 -- Epoch 9, training loss 1.6491420532989503\n",
      "Epoch 10, Batch 0 -- Epoch 10, Batch 100 -- Epoch 10, Batch 200 -- Epoch 10, Batch 300 -- Epoch 10, Batch 400 -- Epoch 10, Batch 500 -- Epoch 10, Batch 600 -- Epoch 10, training loss 1.607948144569397\n",
      "Epoch 11, Batch 0 -- Epoch 11, Batch 100 -- Epoch 11, Batch 200 -- Epoch 11, Batch 300 -- Epoch 11, Batch 400 -- Epoch 11, Batch 500 -- Epoch 11, Batch 600 -- Epoch 11, training loss 1.530492548904419\n",
      "Epoch 12, Batch 0 -- Epoch 12, Batch 100 -- Epoch 12, Batch 200 -- Epoch 12, Batch 300 -- Epoch 12, Batch 400 -- Epoch 12, Batch 500 -- Epoch 12, Batch 600 -- Epoch 12, training loss 1.4678651077651979\n",
      "Epoch 13, Batch 0 -- Epoch 13, Batch 100 -- Epoch 13, Batch 200 -- Epoch 13, Batch 300 -- Epoch 13, Batch 400 -- Epoch 13, Batch 500 -- Epoch 13, Batch 600 -- Epoch 13, training loss 1.5262599505615235\n",
      "Epoch 14, Batch 0 -- Epoch 14, Batch 100 -- Epoch 14, Batch 200 -- Epoch 14, Batch 300 -- Epoch 14, Batch 400 -- Epoch 14, Batch 500 -- Epoch 14, Batch 600 -- Epoch 14, training loss 1.454731575012207\n",
      "Epoch 15, Batch 0 -- Epoch 15, Batch 100 -- Epoch 15, Batch 200 -- Epoch 15, Batch 300 -- Epoch 15, Batch 400 -- Epoch 15, Batch 500 -- Epoch 15, Batch 600 -- Epoch 15, training loss 1.44164622051239\n",
      "Epoch 16, Batch 0 -- Epoch 16, Batch 100 -- Epoch 16, Batch 200 -- Epoch 16, Batch 300 -- Epoch 16, Batch 400 -- Epoch 16, Batch 500 -- Epoch 16, Batch 600 -- Epoch 16, training loss 1.3485661194992065\n",
      "Epoch 17, Batch 0 -- Epoch 17, Batch 100 -- Epoch 17, Batch 200 -- Epoch 17, Batch 300 -- Epoch 17, Batch 400 -- Epoch 17, Batch 500 -- Epoch 17, Batch 600 -- Epoch 17, training loss 1.3662650746154785\n",
      "Epoch 18, Batch 0 -- Epoch 18, Batch 100 -- Epoch 18, Batch 200 -- Epoch 18, Batch 300 -- Epoch 18, Batch 400 -- Epoch 18, Batch 500 -- Epoch 18, Batch 600 -- Epoch 18, training loss 1.3712902353286742\n",
      "Epoch 19, Batch 0 -- Epoch 19, Batch 100 -- Epoch 19, Batch 200 -- Epoch 19, Batch 300 -- Epoch 19, Batch 400 -- Epoch 19, Batch 500 -- Epoch 19, Batch 600 -- Epoch 19, training loss 1.2162312479400634\n",
      "Epoch 20, Batch 0 -- Epoch 20, Batch 100 -- Epoch 20, Batch 200 -- Epoch 20, Batch 300 -- Epoch 20, Batch 400 -- Epoch 20, Batch 500 -- Epoch 20, Batch 600 -- Epoch 20, training loss 1.2952748628234863\n",
      "Epoch 21, Batch 0 -- Epoch 21, Batch 100 -- Epoch 21, Batch 200 -- Epoch 21, Batch 300 -- Epoch 21, Batch 400 -- Epoch 21, Batch 500 -- Epoch 21, Batch 600 -- Epoch 21, training loss 1.2451625273895264\n",
      "Epoch 22, Batch 0 -- Epoch 22, Batch 100 -- Epoch 22, Batch 200 -- Epoch 22, Batch 300 -- Epoch 22, Batch 400 -- Epoch 22, Batch 500 -- Epoch 22, Batch 600 -- Epoch 22, training loss 1.232300224685669\n",
      "Epoch 23, Batch 0 -- Epoch 23, Batch 100 -- Epoch 23, Batch 200 -- Epoch 23, Batch 300 -- Epoch 23, Batch 400 -- Epoch 23, Batch 500 -- Epoch 23, Batch 600 -- Epoch 23, training loss 1.1772003427124023\n",
      "Epoch 24, Batch 0 -- Epoch 24, Batch 100 -- Epoch 24, Batch 200 -- Epoch 24, Batch 300 -- Epoch 24, Batch 400 -- Epoch 24, Batch 500 -- Epoch 24, Batch 600 -- Epoch 24, training loss 1.2008251787376403\n",
      "Epoch 25, Batch 0 -- Epoch 25, Batch 100 -- Epoch 25, Batch 200 -- Epoch 25, Batch 300 -- Epoch 25, Batch 400 -- Epoch 25, Batch 500 -- Epoch 25, Batch 600 -- Epoch 25, training loss 1.1359723747634887\n",
      "Epoch 26, Batch 0 -- Epoch 26, Batch 100 -- Epoch 26, Batch 200 -- Epoch 26, Batch 300 -- Epoch 26, Batch 400 -- Epoch 26, Batch 500 -- Epoch 26, Batch 600 -- Epoch 26, training loss 1.2006299924087525\n",
      "Epoch 27, Batch 0 -- Epoch 27, Batch 100 -- Epoch 27, Batch 200 -- Epoch 27, Batch 300 -- Epoch 27, Batch 400 -- Epoch 27, Batch 500 -- Epoch 27, Batch 600 -- Epoch 27, training loss 1.1613824407196045\n",
      "Epoch 28, Batch 0 -- Epoch 28, Batch 100 -- Epoch 28, Batch 200 -- Epoch 28, Batch 300 -- Epoch 28, Batch 400 -- Epoch 28, Batch 500 -- Epoch 28, Batch 600 -- Epoch 28, training loss 1.0688251155090331\n",
      "Epoch 29, Batch 0 -- Epoch 29, Batch 100 -- Epoch 29, Batch 200 -- Epoch 29, Batch 300 -- Epoch 29, Batch 400 -- Epoch 29, Batch 500 -- Epoch 29, Batch 600 -- Epoch 29, training loss 1.0560017416763305\n",
      "Epoch 30, Batch 0 -- Epoch 30, Batch 100 -- Epoch 30, Batch 200 -- Epoch 30, Batch 300 -- Epoch 30, Batch 400 -- Epoch 30, Batch 500 -- Epoch 30, Batch 600 -- Epoch 30, training loss 1.0702124534988404\n",
      "Epoch 31, Batch 0 -- Epoch 31, Batch 100 -- Epoch 31, Batch 200 -- Epoch 31, Batch 300 -- Epoch 31, Batch 400 -- Epoch 31, Batch 500 -- Epoch 31, Batch 600 -- Epoch 31, training loss 1.054299521522522\n",
      "Epoch 32, Batch 0 -- Epoch 32, Batch 100 -- Epoch 32, Batch 200 -- Epoch 32, Batch 300 -- Epoch 32, Batch 400 -- Epoch 32, Batch 500 -- Epoch 32, Batch 600 -- Epoch 32, training loss 1.0337387195396424\n",
      "Epoch 33, Batch 0 -- Epoch 33, Batch 100 -- Epoch 33, Batch 200 -- Epoch 33, Batch 300 -- Epoch 33, Batch 400 -- Epoch 33, Batch 500 -- Epoch 33, Batch 600 -- Epoch 33, training loss 1.0080643540000915\n",
      "Epoch 34, Batch 0 -- Epoch 34, Batch 100 -- Epoch 34, Batch 200 -- Epoch 34, Batch 300 -- Epoch 34, Batch 400 -- Epoch 34, Batch 500 -- Epoch 34, Batch 600 -- Epoch 34, training loss 1.0001553545188904\n",
      "Epoch 35, Batch 0 -- Epoch 35, Batch 100 -- Epoch 35, Batch 200 -- Epoch 35, Batch 300 -- Epoch 35, Batch 400 -- Epoch 35, Batch 500 -- Epoch 35, Batch 600 -- Epoch 35, training loss 1.0668830179214477\n",
      "Epoch 36, Batch 0 -- Epoch 36, Batch 100 -- Epoch 36, Batch 200 -- Epoch 36, Batch 300 -- Epoch 36, Batch 400 -- Epoch 36, Batch 500 -- Epoch 36, Batch 600 -- Epoch 36, training loss 0.9978771905899048\n",
      "Epoch 37, Batch 0 -- Epoch 37, Batch 100 -- Epoch 37, Batch 200 -- Epoch 37, Batch 300 -- Epoch 37, Batch 400 -- Epoch 37, Batch 500 -- Epoch 37, Batch 600 -- Epoch 37, training loss 0.9674150288009643\n",
      "Epoch 38, Batch 0 -- Epoch 38, Batch 100 -- Epoch 38, Batch 200 -- Epoch 38, Batch 300 -- Epoch 38, Batch 400 -- Epoch 38, Batch 500 -- Epoch 38, Batch 600 -- Epoch 38, training loss 0.9487705871963501\n",
      "Epoch 39, Batch 0 -- Epoch 39, Batch 100 -- Epoch 39, Batch 200 -- Epoch 39, Batch 300 -- Epoch 39, Batch 400 -- Epoch 39, Batch 500 -- Epoch 39, Batch 600 -- Epoch 39, training loss 0.9357417227745056\n",
      "Epoch 40, Batch 0 -- Epoch 40, Batch 100 -- Epoch 40, Batch 200 -- Epoch 40, Batch 300 -- Epoch 40, Batch 400 -- Epoch 40, Batch 500 -- Epoch 40, Batch 600 -- Epoch 40, training loss 0.9323306232643127\n",
      "Epoch 41, Batch 0 -- Epoch 41, Batch 100 -- Epoch 41, Batch 200 -- Epoch 41, Batch 300 -- Epoch 41, Batch 400 -- Epoch 41, Batch 500 -- Epoch 41, Batch 600 -- Epoch 41, training loss 0.9982332288360596\n",
      "Epoch 42, Batch 0 -- Epoch 42, Batch 100 -- Epoch 42, Batch 200 -- Epoch 42, Batch 300 -- Epoch 42, Batch 400 -- Epoch 42, Batch 500 -- Epoch 42, Batch 600 -- Epoch 42, training loss 0.8929728143310547\n",
      "Epoch 43, Batch 0 -- Epoch 43, Batch 100 -- Epoch 43, Batch 200 -- Epoch 43, Batch 300 -- Epoch 43, Batch 400 -- Epoch 43, Batch 500 -- Epoch 43, Batch 600 -- Epoch 43, training loss 0.882673551940918\n",
      "Epoch 44, Batch 0 -- Epoch 44, Batch 100 -- Epoch 44, Batch 200 -- Epoch 44, Batch 300 -- Epoch 44, Batch 400 -- Epoch 44, Batch 500 -- Epoch 44, Batch 600 -- Epoch 44, training loss 0.8856759068679809\n",
      "Epoch 45, Batch 0 -- Epoch 45, Batch 100 -- Epoch 45, Batch 200 -- Epoch 45, Batch 300 -- Epoch 45, Batch 400 -- Epoch 45, Batch 500 -- Epoch 45, Batch 600 -- Epoch 45, training loss 0.8504641718482971\n",
      "Epoch 46, Batch 0 -- Epoch 46, Batch 100 -- Epoch 46, Batch 200 -- Epoch 46, Batch 300 -- Epoch 46, Batch 400 -- Epoch 46, Batch 500 -- Epoch 46, Batch 600 -- Epoch 46, training loss 0.8584702526855469\n",
      "Epoch 47, Batch 0 -- Epoch 47, Batch 100 -- Epoch 47, Batch 200 -- Epoch 47, Batch 300 -- Epoch 47, Batch 400 -- Epoch 47, Batch 500 -- Epoch 47, Batch 600 -- Epoch 47, training loss 0.8421610871124268\n",
      "Epoch 48, Batch 0 -- Epoch 48, Batch 100 -- Epoch 48, Batch 200 -- Epoch 48, Batch 300 -- Epoch 48, Batch 400 -- Epoch 48, Batch 500 -- Epoch 48, Batch 600 -- Epoch 48, training loss 0.9227708383369446\n",
      "Epoch 49, Batch 0 -- Epoch 49, Batch 100 -- Epoch 49, Batch 200 -- Epoch 49, Batch 300 -- Epoch 49, Batch 400 -- Epoch 49, Batch 500 -- Epoch 49, Batch 600 -- Epoch 49, training loss 0.8004405127334595\n",
      "Epoch 50, Batch 0 -- Epoch 50, Batch 100 -- Epoch 50, Batch 200 -- Epoch 50, Batch 300 -- Epoch 50, Batch 400 -- Epoch 50, Batch 500 -- Epoch 50, Batch 600 -- Epoch 50, training loss 0.8128330525779724\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.train()\n",
    "model.to(DEVICE)\n",
    "n_epochs = 50\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch_id, (X, y) in enumerate(train_loader_final):\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_id % 100 == 0:\n",
    "            print('Epoch {}, Batch {}'.format(epoch+1, batch_id), end=\" -- \")\n",
    "    epoch_train_loss = train_loss / len(train_loader_final.dataset)\n",
    "    print('Epoch {}, training loss {}'.format(epoch+1, epoch_train_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657cfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = np.load('embeddings_test.npy')\n",
    "# embeddings_test = (embeddings_test - np.mean(embeddings_test, axis=1)[:, np.newaxis]) / np.std(embeddings_test, axis=1)[:, np.newaxis]\n",
    "\n",
    "X_test = embeddings_test\n",
    "test_loader = create_loader_from_np(X_test, train=False, batch_size=BATCH_SIZE, shuffle=False)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e56bba5fa2d905",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for [x_batch] in test_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        output = torch.flatten(model(x_batch))\n",
    "        output = output.cpu().numpy()[:, np.newaxis]\n",
    "        results.append(np.clip(output, a_min=0, a_max=10))\n",
    "    results = np.vstack(results)\n",
    "\n",
    "    with open(\"result.txt\", \"w\") as f:\n",
    "        for val in np.concatenate(results):\n",
    "            f.write(f\"{val}\\n\")\n",
    "print(\"Results saved to results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
