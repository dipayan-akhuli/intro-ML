{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16163d2dd773fbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1a3a9db8e3f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824a840beb8b323e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82adb41ca8c23be6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04a48e",
   "metadata": {},
   "source": [
    "#### Extracting image embeddings using pre-trained Vision Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3d5c760c9c963b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract the embeddings.\n",
    "\"\"\"\n",
    "# transform images\n",
    "batch = 64\n",
    "train_transforms = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms()\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch, shuffle=False, pin_memory=True, num_workers=10)\n",
    "\n",
    "# extract embeddings using pre-trained model\n",
    "model = vit_l_16(weights=ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
    "embedding_size = model.heads[0].in_features\n",
    "num_images = len(train_dataset)\n",
    "embeddings = np.zeros((num_images, embedding_size))\n",
    "model.heads[0] = nn.Identity()\n",
    "model.to(device)\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        embeddings[batch*i : batch*(i+1)] = outputs.cpu().numpy()\n",
    "        del inputs\n",
    "        del outputs\n",
    "        print(i, end=\"--\")\n",
    "        i += 1\n",
    "np.save('dataset/embeddings-ViT_L_16.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c7bfc",
   "metadata": {},
   "source": [
    "#### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335d91cc379d4f6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "           train: boolean, whether the data is for training or testing\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings-ViT_L_16.npy')\n",
    "    # normalize embeddings\n",
    "    embeddings = (embeddings - np.mean(embeddings, axis=1)[:, np.newaxis]) / np.std(embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daf836a4adb0abe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y=None, train=True, batch_size=batch, shuffle=True, num_workers=10):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1b0092e0b13f88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "train_loader = create_loader_from_np(X[0:round(0.8*X.shape[0])], y[0:round(0.8*len(y))], train=True, batch_size=batch)\n",
    "valid_loader = create_loader_from_np(X[round(0.8*X.shape[0]):], y[round(0.8*len(y)):], train=True, batch_size=batch)\n",
    "train_loader_final = create_loader_from_np(X, y, train=True, batch_size=batch)\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1baa5918f11a049",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Defining neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e6e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 200\n",
    "layer2_size = 200\n",
    "dropout_prop = 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*embedding_size, layer1_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prop)\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_prop)\n",
    "        self.fc3 = nn.Linear(layer2_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44343c0b",
   "metadata": {},
   "source": [
    "#### Training and evaluating neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28634c90281cd699",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0 -- Epoch 1, Batch 300 -- Epoch 1, Batch 600 -- Epoch 1, Batch 900 -- Epoch 1, Batch 1200 -- Epoch 1, training loss 0.5557088509613873, accuracy: (71%)\n",
      "Epoch 1, validation loss 0.4968480159182173, accuracy: (75%)\n",
      "Epoch 2, Batch 0 -- Epoch 2, Batch 300 -- Epoch 2, Batch 600 -- Epoch 2, Batch 900 -- Epoch 2, Batch 1200 -- Epoch 2, training loss 0.5037619577208897, accuracy: (75%)\n",
      "Epoch 2, validation loss 0.4866225994991313, accuracy: (77%)\n",
      "Epoch 3, Batch 0 -- Epoch 3, Batch 300 -- Epoch 3, Batch 600 -- Epoch 3, Batch 900 -- Epoch 3, Batch 1200 -- Epoch 3, training loss 0.4780532561019119, accuracy: (77%)\n",
      "Epoch 3, validation loss 0.4714181385780997, accuracy: (77%)\n",
      "Epoch 4, Batch 0 -- Epoch 4, Batch 300 -- Epoch 4, Batch 600 -- Epoch 4, Batch 900 -- Epoch 4, Batch 1200 -- Epoch 4, training loss 0.4530173528347658, accuracy: (78%)\n",
      "Epoch 4, validation loss 0.44862223434906934, accuracy: (79%)\n",
      "Epoch 5, Batch 0 -- Epoch 5, Batch 300 -- Epoch 5, Batch 600 -- Epoch 5, Batch 900 -- Epoch 5, Batch 1200 -- Epoch 5, training loss 0.4268114263046131, accuracy: (80%)\n",
      "Epoch 5, validation loss 0.4346931385165712, accuracy: (80%)\n",
      "Epoch 6, Batch 0 -- Epoch 6, Batch 300 -- Epoch 6, Batch 600 -- Epoch 6, Batch 900 -- Epoch 6, Batch 1200 -- Epoch 6, training loss 0.4002221289999914, accuracy: (82%)\n",
      "Epoch 6, validation loss 0.4237309271228881, accuracy: (81%)\n",
      "Epoch 7, Batch 0 -- Epoch 7, Batch 300 -- Epoch 7, Batch 600 -- Epoch 7, Batch 900 -- Epoch 7, Batch 1200 -- Epoch 7, training loss 0.3725787317298154, accuracy: (83%)\n",
      "Epoch 7, validation loss 0.40104337225518766, accuracy: (82%)\n",
      "Epoch 8, Batch 0 -- Epoch 8, Batch 300 -- Epoch 8, Batch 600 -- Epoch 8, Batch 900 -- Epoch 8, Batch 1200 -- Epoch 8, training loss 0.35074436214404237, accuracy: (84%)\n",
      "Epoch 8, validation loss 0.3927268674771625, accuracy: (82%)\n",
      "Epoch 9, Batch 0 -- Epoch 9, Batch 300 -- Epoch 9, Batch 600 -- Epoch 9, Batch 900 -- Epoch 9, Batch 1200 -- Epoch 9, training loss 0.3262100379842375, accuracy: (86%)\n",
      "Epoch 9, validation loss 0.3776008708431151, accuracy: (83%)\n",
      "Epoch 10, Batch 0 -- Epoch 10, Batch 300 -- Epoch 10, Batch 600 -- Epoch 10, Batch 900 -- Epoch 10, Batch 1200 -- Epoch 10, training loss 0.3019902011176012, accuracy: (87%)\n",
      "Epoch 10, validation loss 0.36930076838400966, accuracy: (84%)\n",
      "Epoch 11, Batch 0 -- Epoch 11, Batch 300 -- Epoch 11, Batch 600 -- Epoch 11, Batch 900 -- Epoch 11, Batch 1200 -- Epoch 11, training loss 0.28494622610871817, accuracy: (88%)\n",
      "Epoch 11, validation loss 0.3559784826576975, accuracy: (84%)\n",
      "Epoch 12, Batch 0 -- Epoch 12, Batch 300 -- Epoch 12, Batch 600 -- Epoch 12, Batch 900 -- Epoch 12, Batch 1200 -- Epoch 12, training loss 0.2686245844467922, accuracy: (88%)\n",
      "Epoch 12, validation loss 0.3610377231135365, accuracy: (85%)\n",
      "Epoch 13, Batch 0 -- Epoch 13, Batch 300 -- Epoch 13, Batch 600 -- Epoch 13, Batch 900 -- Epoch 13, Batch 1200 -- Epoch 13, training loss 0.253541265977768, accuracy: (89%)\n",
      "Epoch 13, validation loss 0.349571307241481, accuracy: (85%)\n",
      "Epoch 14, Batch 0 -- Epoch 14, Batch 300 -- Epoch 14, Batch 600 -- Epoch 14, Batch 900 -- Epoch 14, Batch 1200 -- Epoch 14, training loss 0.23906904475023733, accuracy: (90%)\n",
      "Epoch 14, validation loss 0.33932873325508944, accuracy: (86%)\n",
      "Epoch 15, Batch 0 -- Epoch 15, Batch 300 -- Epoch 15, Batch 600 -- Epoch 15, Batch 900 -- Epoch 15, Batch 1200 -- Epoch 15, training loss 0.22689420716918576, accuracy: (91%)\n",
      "Epoch 15, validation loss 0.34224392798310876, accuracy: (86%)\n",
      "Epoch 16, Batch 0 -- Epoch 16, Batch 300 -- Epoch 16, Batch 600 -- Epoch 16, Batch 900 -- Epoch 16, Batch 1200 -- Epoch 16, training loss 0.21482963812588585, accuracy: (91%)\n",
      "Epoch 16, validation loss 0.33860732909562075, accuracy: (86%)\n",
      "Epoch 17, Batch 0 -- Epoch 17, Batch 300 -- Epoch 17, Batch 600 -- Epoch 17, Batch 900 -- Epoch 17, Batch 1200 -- Epoch 17, training loss 0.20433005722622846, accuracy: (92%)\n",
      "Epoch 17, validation loss 0.3350765752655152, accuracy: (87%)\n",
      "Epoch 18, Batch 0 -- Epoch 18, Batch 300 -- Epoch 18, Batch 600 -- Epoch 18, Batch 900 -- Epoch 18, Batch 1200 -- Epoch 18, training loss 0.19399440392449976, accuracy: (92%)\n",
      "Epoch 18, validation loss 0.33159568914343185, accuracy: (87%)\n",
      "Epoch 19, Batch 0 -- Epoch 19, Batch 300 -- Epoch 19, Batch 600 -- Epoch 19, Batch 900 -- Epoch 19, Batch 1200 -- Epoch 19, training loss 0.1877760279594745, accuracy: (92%)\n",
      "Epoch 19, validation loss 0.3260281265387863, accuracy: (87%)\n",
      "Epoch 20, Batch 0 -- Epoch 20, Batch 300 -- Epoch 20, Batch 600 -- Epoch 20, Batch 900 -- Epoch 20, Batch 1200 -- Epoch 20, training loss 0.17936626842349834, accuracy: (93%)\n",
      "Epoch 20, validation loss 0.31561941323806725, accuracy: (87%)\n",
      "Epoch 21, Batch 0 -- Epoch 21, Batch 300 -- Epoch 21, Batch 600 -- Epoch 21, Batch 900 -- Epoch 21, Batch 1200 -- Epoch 21, training loss 0.16980012260439376, accuracy: (93%)\n",
      "Epoch 21, validation loss 0.3285051286683747, accuracy: (88%)\n",
      "Epoch 22, Batch 0 -- Epoch 22, Batch 300 -- Epoch 22, Batch 600 -- Epoch 22, Batch 900 -- Epoch 22, Batch 1200 -- Epoch 22, training loss 0.16538687353173015, accuracy: (93%)\n",
      "Epoch 22, validation loss 0.31709896845270946, accuracy: (88%)\n",
      "Epoch 23, Batch 0 -- Epoch 23, Batch 300 -- Epoch 23, Batch 600 -- Epoch 23, Batch 900 -- Epoch 23, Batch 1200 -- Epoch 23, training loss 0.15905763710245188, accuracy: (93%)\n",
      "Epoch 23, validation loss 0.3151085417976522, accuracy: (88%)\n",
      "Early stopping after 23 epochs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training procedure of the model; it accepts the training data, defines the model and then trains it.\n",
    "input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "compute: model: torch.nn.Module, the trained model\n",
    "\"\"\"\n",
    "model = Net()\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "n_epochs = 25\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "learn_rate = 0.0003\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    for batch_id, (X, y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.round(F.sigmoid(torch.flatten(output)))\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        train_corrects += torch.sum(preds == y.data)\n",
    "        if batch_id % 300 == 0:\n",
    "            print('Epoch {}, Batch {}'.format(epoch+1, batch_id), end=\" -- \")\n",
    "    epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc = 100 * train_corrects.cpu() / len(train_loader.dataset)\n",
    "    print('Epoch {}, training loss {}, accuracy: ({:.0f}%)'.format(epoch+1, epoch_train_loss, epoch_train_acc))\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader: \n",
    "            model.eval()\n",
    "            X = X.to(device)   \n",
    "            y = y.to(device)        \n",
    "            output_valid = model(X)\n",
    "            loss_valid = loss_function(torch.flatten(output_valid), y)\n",
    "            preds_valid = torch.round(F.sigmoid(torch.flatten(output_valid)))\n",
    "            valid_loss += loss_valid.item() * X.size(0)\n",
    "            valid_corrects += torch.sum(preds_valid == y.data)\n",
    "    epoch_valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    epoch_valid_acc = 100 * valid_corrects.cpu() / len(valid_loader.dataset)\n",
    "    print('Epoch {}, validation loss {}, accuracy: ({:.0f}%)'.format(epoch+1, epoch_valid_loss, epoch_valid_acc))\n",
    "\n",
    "    if epoch_valid_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = epoch_valid_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335eb5c",
   "metadata": {},
   "source": [
    "#### Training final neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 15\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    for batch_id, (X, y) in enumerate(train_loader_final):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.round(F.sigmoid(torch.flatten(output)))\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        train_corrects += torch.sum(preds == y.data)\n",
    "        if batch_id % 400 == 0:\n",
    "            print('Epoch {}, Batch {}'.format(epoch+1, batch_id), end=\" -- \")\n",
    "    epoch_train_loss = train_loss / len(train_loader_final.dataset)\n",
    "    epoch_train_acc = 100 * train_corrects.cpu() / len(train_loader_final.dataset)\n",
    "    print('Epoch {}, training loss {}, accuracy: ({:.0f}%)'.format(epoch+1, epoch_train_loss, epoch_train_acc))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62b50d",
   "metadata": {},
   "source": [
    "#### Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e42ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n",
    "test_loader = create_loader_from_np(X_test, train=False, batch_size=2048, shuffle=False)\n",
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ea99b26c348253",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The testing procedure of the model; it accepts the testing data and the trained model and then tests the model on it.\n",
    "input: model: torch.nn.Module, the trained model\n",
    "       loader: torch.data.util.DataLoader, the object containing the testing data     \n",
    "compute: None, the function saves the predictions to a results.txt file\n",
    "\"\"\"\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for [x_batch] in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        predicted = F.sigmoid(torch.flatten(model(x_batch)))\n",
    "        predicted = predicted.cpu().numpy()[:, np.newaxis]\n",
    "        predicted[predicted >= 0.5] = 1\n",
    "        predicted[predicted < 0.5] = 0\n",
    "        predictions.append(predicted)\n",
    "    predictions = np.vstack(predictions)\n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
