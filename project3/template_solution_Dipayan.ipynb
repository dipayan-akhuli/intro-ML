{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16163d2dd773fbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1a3a9db8e3f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a840beb8b323e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "# from torchvision.models import convnext_small, ConvNeXt_Small_Weights\n",
    "from torchvision.models import regnet_y_16gf, RegNet_Y_16GF_Weights\n",
    "# from torchvision.models import regnet_y_128gf, RegNet_Y_128GF_Weights\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82adb41ca8c23be6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The device is automatically set to GPU if available, otherwise CPU\n",
    "# If you want to force the device to CPU, you can change the line to\n",
    "# device = torch.device(\"cpu\")\n",
    "# When using the GPU, it is important that your model and all data are on the \n",
    "# same device.\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d5c760c9c963b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "the embeddings.\n",
    "\"\"\"\n",
    "# TODO: define a transform to pre-process the images\n",
    "# The required pre-processing depends on the pre-trained model you choose \n",
    "# below. \n",
    "# See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n",
    "train_transforms = RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1.transforms()\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "# run out of memory (VRAM if on GPU, RAM if on CPU)\n",
    "batch = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=10)\n",
    "\n",
    "# TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "# more info here: https://pytorch.org/vision/stable/models.html)\n",
    "model = regnet_y_16gf(weights=RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "# print(model)\n",
    "embedding_size = 3024 # Dummy variable, replace with the actual embedding size once you pick your model\n",
    "num_images = len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96517bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = np.zeros((num_images, embedding_size))\n",
    "# # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "# # model to access the embeddings the model generates. \n",
    "# model.fc = nn.Identity()\n",
    "# # model.classifier[2] = nn.Identity()\n",
    "# model.to(device)\n",
    "# i = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, _ in train_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         embeddings[batch*i : batch*(i+1)] = outputs.cpu().numpy()\n",
    "#         del inputs\n",
    "#         del outputs\n",
    "#         print(i, end=\"--\")\n",
    "#         i += 1\n",
    "# np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d91cc379d4f6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "           train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                         transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    # TODO: Normalize the embeddings\n",
    "    embeddings = (embeddings - np.mean(embeddings, axis=1)[:, np.newaxis]) / np.std(embeddings, axis=1)[:, np.newaxis]\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf836a4adb0abe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y = None, train = True, batch_size=batch, shuffle=True, num_workers = 10):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # Attention: If you get type errors you can modify the type of the\n",
    "        # labels here\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b0092e0b13f88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "# load the training data\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "# Create data loaders for the training data\n",
    "train_loader = create_loader_from_np(X[0:round(0.8*X.shape[0])], y[0:round(0.8*len(y))], train = True, batch_size=batch)\n",
    "valid_loader = create_loader_from_np(X[round(0.8*X.shape[0]):], y[round(0.8*len(y)):], train = True, batch_size=batch)\n",
    "train_loader_final = create_loader_from_np(X, y, train = True, batch_size=batch)\n",
    "# delete the loaded training data to save memory, as the data loader copies\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1baa5918f11a049",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: define a model. Here, the basic structure is defined, but you need to fill in the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6e6e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 1024\n",
    "layer2_size = 64\n",
    "dropout_prop = 0.3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout0 = nn.Dropout(dropout_prop)\n",
    "\n",
    "        self.fc1 = nn.Linear(3*embedding_size, layer1_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.bn1 = nn.BatchNorm1d(layer1_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prop)\n",
    "\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.bn2 = nn.BatchNorm1d(layer2_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_prop)\n",
    "        \n",
    "        self.fc3 = nn.Linear(layer2_size, 1)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28634c90281cd699",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch id 0, training loss 0.8238517045974731\n",
      "Epoch 1, Batch id 200, training loss 0.5929189920425415\n",
      "Epoch 1, Batch id 400, training loss 0.6181861162185669\n",
      "Epoch 1, Batch id 600, training loss 0.5212967395782471\n",
      "Epoch 1, Batch id 800, training loss 0.506117582321167\n",
      "Epoch 1, Batch id 1000, training loss 0.4672303795814514\n",
      "Epoch 1, Batch id 1200, training loss 0.5662831664085388\n",
      "Epoch 1, Batch id 1400, training loss 0.5545528531074524\n",
      "Epoch 1, valid loss 0.5430616140365601\n",
      "Epoch 1, accuracy: (70%)\n",
      "Epoch 2, Batch id 0, training loss 0.4670337438583374\n",
      "Epoch 2, Batch id 200, training loss 0.6115789413452148\n",
      "Epoch 2, Batch id 400, training loss 0.6194772720336914\n",
      "Epoch 2, Batch id 600, training loss 0.5318030714988708\n",
      "Epoch 2, Batch id 800, training loss 0.51119065284729\n",
      "Epoch 2, Batch id 1000, training loss 0.5637558698654175\n",
      "Epoch 2, Batch id 1200, training loss 0.43741393089294434\n",
      "Epoch 2, Batch id 1400, training loss 0.4386410713195801\n",
      "Epoch 2, valid loss 0.524290919303894\n",
      "Epoch 2, accuracy: (72%)\n",
      "Epoch 3, Batch id 0, training loss 0.4507450759410858\n",
      "Epoch 3, Batch id 200, training loss 0.4459517002105713\n",
      "Epoch 3, Batch id 400, training loss 0.5273343920707703\n",
      "Epoch 3, Batch id 600, training loss 0.47789233922958374\n",
      "Epoch 3, Batch id 800, training loss 0.5398110151290894\n",
      "Epoch 3, Batch id 1000, training loss 0.4396139085292816\n",
      "Epoch 3, Batch id 1200, training loss 0.6207824945449829\n",
      "Epoch 3, Batch id 1400, training loss 0.46308010816574097\n",
      "Epoch 3, valid loss 0.5131940245628357\n",
      "Epoch 3, accuracy: (73%)\n",
      "Epoch 4, Batch id 0, training loss 0.3943566381931305\n",
      "Epoch 4, Batch id 200, training loss 0.5945450067520142\n",
      "Epoch 4, Batch id 400, training loss 0.5492031574249268\n",
      "Epoch 4, Batch id 600, training loss 0.47551053762435913\n",
      "Epoch 4, Batch id 800, training loss 0.4961186945438385\n",
      "Epoch 4, Batch id 1000, training loss 0.45149219036102295\n",
      "Epoch 4, Batch id 1200, training loss 0.49053171277046204\n",
      "Epoch 4, Batch id 1400, training loss 0.6627790927886963\n",
      "Epoch 4, valid loss 0.49999499320983887\n",
      "Epoch 4, accuracy: (74%)\n",
      "Epoch 5, Batch id 0, training loss 0.43560507893562317\n",
      "Epoch 5, Batch id 200, training loss 0.38674047589302063\n",
      "Epoch 5, Batch id 400, training loss 0.37275925278663635\n",
      "Epoch 5, Batch id 600, training loss 0.6594719886779785\n",
      "Epoch 5, Batch id 800, training loss 0.5345987677574158\n",
      "Epoch 5, Batch id 1000, training loss 0.5511389970779419\n",
      "Epoch 5, Batch id 1200, training loss 0.5008080005645752\n",
      "Epoch 5, Batch id 1400, training loss 0.5197615623474121\n",
      "Epoch 5, valid loss 0.49079787731170654\n",
      "Epoch 5, accuracy: (75%)\n",
      "Epoch 6, Batch id 0, training loss 0.4638219177722931\n",
      "Epoch 6, Batch id 200, training loss 0.516249418258667\n",
      "Epoch 6, Batch id 400, training loss 0.41747164726257324\n",
      "Epoch 6, Batch id 600, training loss 0.43875548243522644\n",
      "Epoch 6, Batch id 800, training loss 0.36534595489501953\n",
      "Epoch 6, Batch id 1000, training loss 0.45094504952430725\n",
      "Epoch 6, Batch id 1200, training loss 0.38560497760772705\n",
      "Epoch 6, Batch id 1400, training loss 0.515271782875061\n",
      "Epoch 6, valid loss 0.479445219039917\n",
      "Epoch 6, accuracy: (75%)\n",
      "Epoch 7, Batch id 0, training loss 0.3371175527572632\n",
      "Epoch 7, Batch id 200, training loss 0.34316933155059814\n",
      "Epoch 7, Batch id 400, training loss 0.3229430913925171\n",
      "Epoch 7, Batch id 600, training loss 0.4251161217689514\n",
      "Epoch 7, Batch id 800, training loss 0.37549304962158203\n",
      "Epoch 7, Batch id 1000, training loss 0.4528122544288635\n",
      "Epoch 7, Batch id 1200, training loss 0.399673193693161\n",
      "Epoch 7, Batch id 1400, training loss 0.5421478748321533\n",
      "Epoch 7, valid loss 0.47281399369239807\n",
      "Epoch 7, accuracy: (76%)\n",
      "Epoch 8, Batch id 0, training loss 0.3916175067424774\n",
      "Epoch 8, Batch id 200, training loss 0.4285210967063904\n",
      "Epoch 8, Batch id 400, training loss 0.43393266201019287\n",
      "Epoch 8, Batch id 600, training loss 0.44055479764938354\n",
      "Epoch 8, Batch id 800, training loss 0.4555213451385498\n",
      "Epoch 8, Batch id 1000, training loss 0.614762008190155\n",
      "Epoch 8, Batch id 1200, training loss 0.31414198875427246\n",
      "Epoch 8, Batch id 1400, training loss 0.4413166046142578\n",
      "Epoch 8, valid loss 0.4706895351409912\n",
      "Epoch 8, accuracy: (77%)\n",
      "Early stopping after 8 epochs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training procedure of the model; it accepts the training data, defines the model \n",
    "and then trains it.\n",
    "\n",
    "input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "compute: model: torch.nn.Module, the trained model\n",
    "\"\"\"\n",
    "model = Net()\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 10\n",
    "patience = 2\n",
    "min_delta = 0.01\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "# TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "# of the training data as a validation split. After each epoch, compute the loss on the \n",
    "# validation split and print it out. This enables you to see how your model is performing \n",
    "# on the validation data before submitting the results on the server. After choosing the \n",
    "# best model, train it on the whole training data.\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "L2_lambda = 0.0002\n",
    "learn_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate, weight_decay=L2_lambda)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for batch_id, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 200 == 0:\n",
    "            print('Epoch {}, Batch id {}, training loss {}'.format(epoch+1, batch_id, loss.item()))\n",
    "\n",
    "    valid_losses = []\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader: \n",
    "            X = X.to(device)   \n",
    "            y = y.to(device)        \n",
    "            output_valid = model(X)\n",
    "            valid_loss = loss_function(torch.flatten(output_valid), y).cpu().numpy()\n",
    "            valid_losses.append(valid_loss)\n",
    "            output_valid = output_valid.cpu().numpy()\n",
    "            output_valid[output_valid >= 0.5] = 1\n",
    "            output_valid[output_valid < 0.5] = 0\n",
    "            y = y.cpu().numpy()[:, np.newaxis]\n",
    "            correct += np.count_nonzero(output_valid == y)\n",
    "        print('Epoch {}, valid loss {}'.format(epoch+1, np.mean(valid_losses)))\n",
    "        print('Epoch {}, accuracy: ({:.0f}%)'.format(epoch+1, 100. * correct / len(valid_loader.dataset)))\n",
    "\n",
    "    if np.mean(valid_losses) < best_val_loss - min_delta:\n",
    "        best_val_loss = np.mean(valid_losses)\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Check if early stopping criteria met\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5da3242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch id 0, training loss 1.0154438018798828\n",
      "Epoch 1, Batch id 200, training loss 0.5264649391174316\n",
      "Epoch 1, Batch id 400, training loss 0.5525493621826172\n",
      "Epoch 1, Batch id 600, training loss 0.5857006907463074\n",
      "Epoch 1, Batch id 800, training loss 0.6069153547286987\n",
      "Epoch 1, Batch id 1000, training loss 0.5840517282485962\n",
      "Epoch 1, Batch id 1200, training loss 0.5669487714767456\n",
      "Epoch 1, Batch id 1400, training loss 0.5469427108764648\n",
      "Epoch 1, Batch id 1600, training loss 0.4686805009841919\n",
      "Epoch 1, Batch id 1800, training loss 0.42942625284194946\n",
      "Epoch 2, Batch id 0, training loss 0.4790855646133423\n",
      "Epoch 2, Batch id 200, training loss 0.5497839450836182\n",
      "Epoch 2, Batch id 400, training loss 0.570398211479187\n",
      "Epoch 2, Batch id 600, training loss 0.48341870307922363\n",
      "Epoch 2, Batch id 800, training loss 0.49982285499572754\n",
      "Epoch 2, Batch id 1000, training loss 0.5324530005455017\n",
      "Epoch 2, Batch id 1200, training loss 0.513380765914917\n",
      "Epoch 2, Batch id 1400, training loss 0.4732412099838257\n",
      "Epoch 2, Batch id 1600, training loss 0.48143118619918823\n",
      "Epoch 2, Batch id 1800, training loss 0.45614176988601685\n",
      "Epoch 3, Batch id 0, training loss 0.46347856521606445\n",
      "Epoch 3, Batch id 200, training loss 0.540372908115387\n",
      "Epoch 3, Batch id 400, training loss 0.38303476572036743\n",
      "Epoch 3, Batch id 600, training loss 0.5410866737365723\n",
      "Epoch 3, Batch id 800, training loss 0.3789360523223877\n",
      "Epoch 3, Batch id 1000, training loss 0.5140658617019653\n",
      "Epoch 3, Batch id 1200, training loss 0.4635036587715149\n",
      "Epoch 3, Batch id 1400, training loss 0.4395080804824829\n",
      "Epoch 3, Batch id 1600, training loss 0.5544854402542114\n",
      "Epoch 3, Batch id 1800, training loss 0.5030308961868286\n",
      "Epoch 4, Batch id 0, training loss 0.4246100187301636\n",
      "Epoch 4, Batch id 200, training loss 0.4761851131916046\n",
      "Epoch 4, Batch id 400, training loss 0.5129543542861938\n",
      "Epoch 4, Batch id 600, training loss 0.5067392587661743\n",
      "Epoch 4, Batch id 800, training loss 0.4158599376678467\n",
      "Epoch 4, Batch id 1000, training loss 0.5070855617523193\n",
      "Epoch 4, Batch id 1200, training loss 0.48446109890937805\n",
      "Epoch 4, Batch id 1400, training loss 0.3800079822540283\n",
      "Epoch 4, Batch id 1600, training loss 0.50969398021698\n",
      "Epoch 4, Batch id 1800, training loss 0.4882672131061554\n",
      "Epoch 5, Batch id 0, training loss 0.38351038098335266\n",
      "Epoch 5, Batch id 200, training loss 0.49476203322410583\n",
      "Epoch 5, Batch id 400, training loss 0.514693021774292\n",
      "Epoch 5, Batch id 600, training loss 0.30309993028640747\n",
      "Epoch 5, Batch id 800, training loss 0.5786451697349548\n",
      "Epoch 5, Batch id 1000, training loss 0.5438985228538513\n",
      "Epoch 5, Batch id 1200, training loss 0.5978066921234131\n",
      "Epoch 5, Batch id 1400, training loss 0.6574503183364868\n",
      "Epoch 5, Batch id 1600, training loss 0.5718731880187988\n",
      "Epoch 5, Batch id 1800, training loss 0.39624762535095215\n",
      "Epoch 6, Batch id 0, training loss 0.3821706473827362\n",
      "Epoch 6, Batch id 200, training loss 0.3233228325843811\n",
      "Epoch 6, Batch id 400, training loss 0.41004854440689087\n",
      "Epoch 6, Batch id 600, training loss 0.4062737822532654\n",
      "Epoch 6, Batch id 800, training loss 0.4671538174152374\n",
      "Epoch 6, Batch id 1000, training loss 0.47980374097824097\n",
      "Epoch 6, Batch id 1200, training loss 0.458585262298584\n",
      "Epoch 6, Batch id 1400, training loss 0.4294150471687317\n",
      "Epoch 6, Batch id 1600, training loss 0.4169979691505432\n",
      "Epoch 6, Batch id 1800, training loss 0.6085736751556396\n",
      "Epoch 7, Batch id 0, training loss 0.3454214930534363\n",
      "Epoch 7, Batch id 200, training loss 0.4355360269546509\n",
      "Epoch 7, Batch id 400, training loss 0.44750720262527466\n",
      "Epoch 7, Batch id 600, training loss 0.3443118929862976\n",
      "Epoch 7, Batch id 800, training loss 0.3666757345199585\n",
      "Epoch 7, Batch id 1000, training loss 0.41711947321891785\n",
      "Epoch 7, Batch id 1200, training loss 0.3969767689704895\n",
      "Epoch 7, Batch id 1400, training loss 0.38702476024627686\n",
      "Epoch 7, Batch id 1600, training loss 0.4758365750312805\n",
      "Epoch 7, Batch id 1800, training loss 0.368638813495636\n",
      "Epoch 8, Batch id 0, training loss 0.40371304750442505\n",
      "Epoch 8, Batch id 200, training loss 0.39688897132873535\n",
      "Epoch 8, Batch id 400, training loss 0.5681624412536621\n",
      "Epoch 8, Batch id 600, training loss 0.4052387475967407\n",
      "Epoch 8, Batch id 800, training loss 0.5965464115142822\n",
      "Epoch 8, Batch id 1000, training loss 0.34982192516326904\n",
      "Epoch 8, Batch id 1200, training loss 0.39551377296447754\n",
      "Epoch 8, Batch id 1400, training loss 0.37302201986312866\n",
      "Epoch 8, Batch id 1600, training loss 0.39336565136909485\n",
      "Epoch 8, Batch id 1800, training loss 0.34560197591781616\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 8\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate, weight_decay=L2_lambda)\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_id, (X, y) in enumerate(train_loader_final):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X)\n",
    "        loss = loss_function(torch.flatten(output), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 200 == 0:\n",
    "            print('Epoch {}, Batch id {}, training loss {}'.format(epoch+1, batch_id, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "# repeat for testing data\n",
    "X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2ea99b26c348253",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "then tests the model on it.\n",
    "\n",
    "input: model: torch.nn.Module, the trained model\n",
    "       loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "compute: None, the function saves the predictions to a results.txt file\n",
    "\"\"\"\n",
    "model.eval()\n",
    "predictions = []\n",
    "preds_orig = []\n",
    "# Iterate over the test data\n",
    "with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "    for [x_batch] in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        predicted_vals = model(x_batch)\n",
    "        predicted = predicted_vals.cpu().numpy()\n",
    "        pred_orig = predicted_vals.cpu().numpy()\n",
    "        preds_orig.append(pred_orig)\n",
    "        # Rounding the predictions to 0 or 1\n",
    "        predicted[predicted >= 0.5] = 1\n",
    "        predicted[predicted < 0.5] = 0\n",
    "        predictions.append(predicted)\n",
    "    predictions = np.vstack(predictions)\n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
